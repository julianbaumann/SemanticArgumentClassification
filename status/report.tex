\documentclass[runningheads]{llncs}

\usepackage[margin=1.5in]{geometry}
\usepackage{ngerman}
\usepackage{gantt}
\usepackage{listings}
\usepackage{cite}
\usepackage{hyperref}

\definecolor{airforce}{rgb}{0.36, 0.54, 0.66}
\definecolor{amber}{rgb}{1.0, 0.75, 0.0}
\definecolor{pastelgreen}{rgb}{0.01, 0.75, 0.24}
\definecolor{deepcarrot}{rgb}{0.91, 0.41, 0.17}

\begin{document}

\lstset{language=Python}

\title{Semantic Argument Classification}

\author{Julian Baumann, Kevin Decker, Maximilian M\"uller-Eberstein}

\institute{Ruprecht-Karls-Universit\"at Heidelberg}

\maketitle

\section{Einleitung}
\textit{Wem} ist durch \textit{wen}, \textit{was}, \textit{wo}, \textit{wie}, \textit{wann} widerfahren? Genau diese Fragen beantwortet die semantische Argumentklassifikation. Den Argumenten des Verbs ihre semantischen Rollen zuzuweisen ist f\"ur das tiefere Verst\"andnis textueller Daten unumg\"anglich. Um diese Aufgabe bestm\"oglich zu l\"osen, testen wir verschiedene Klassifikationsalgorithmen und orientieren uns dabei an \textit{Support Vector Learning for Semantic Argument Classiﬁcation}\cite{svm}.
\subsection{Organisatorisches}
\begin{gantt}{13}{10}
    \begin{ganttitle}
      \titleelement{Oktober}{1}
      \titleelement{November}{2}
      \titleelement{Dezember}{3}
      \titleelement{Januar}{2}
      \titleelement{Februar}{2}
    \end{ganttitle}
    \begin{ganttitle}
      \titleelement{22.10.}{1}
      \titleelement{05.11.}{1}
      \titleelement{19.11.}{1}
      \titleelement{3.12.}{1}
      \titleelement{17.12.}{1}
      \titleelement{31.12.}{1}
      \titleelement{14.1.}{1}
      \titleelement{28.1.}{1}
      \titleelement{11.2.}{1}
      \titleelement{28.2.}{1}
    \end{ganttitle}
    \ganttbar[color=pastelgreen]{Team \& Thema}{0}{1}
    \ganttbar[color=pastelgreen]{Paper \& Ressourcen}{1}{1}
    \ganttbar[color=pastelgreen]{Umsetzungsplanung}{2}{2}
    \ganttmilestone[color=airforce]{Statusbericht}{4}
    \ganttbar[color=deepcarrot]{Feature-Extraktion}{4}{3}
    \ganttbar[color=deepcarrot]{Experimente (Weka)}{6}{2}
    \ganttmilestone[color=airforce]{Grundarchitektur +1 Feature}{6}
    \ganttbar[color=pastelgreen]{Pr\"asentation vorbereiten}{6}{1}
    \ganttbar[color=pastelgreen]{Endbericht und Dokumentation}{6}{2}
    \ganttmilestone[color=airforce]{Endbericht}{9}
    \ganttbar[color=amber]{Puffer}{8}{1}
\end{gantt}

\section{Grundlagen}

\subsection{Daten}

Als Grundlage f\"ur die oben beschriebenen Experimente wird das \href{http://www.nltk.org/_modules/nltk/corpus/reader/propbank.html}{PropBank-Korpus} aus dem Natural Language Toolkit verwendet. Das urspr\"ungliche Korpus von Martha Palmer et al. \cite{Palmer} umfasst eine gr\"o{\ss}ere Anzahl an PropBank-Instanzen, das im NLTK verwendete besteht allerdings nur aus 112.917 S\"atzen. 
Jedem dieser S\"atze wurden seine entsprechenden Argumente durch Annotatoren zugewiesen. Da die meisten S\"atze mindestens zwei Argumente besitzen (Subjekt und direktes Objekt) umfasst PropBank 292.975 annotierte Argumente. Diese Daten dienen als Goldstandard f\"ur die anschlie{\ss}ende Evaluation des Verfahrens.



\subsection{Tools}
Zur Datenverarbeitung und Auswertung wird die Programmiersprache Python verwendet in der Version 3.4, um die Feature-Extraktion durchzuf\"uhren. Durch die Wahl von Python bietet sich das NLTK als Ressource f\"ur Sprachverarbeitung an, entsprechend in der Version 3.0.0. Als Machine Learning Tool, mit dem die unten aufgef\"uhrten
Algorithmen verwendet werden, wird Weka 3.7.11 eingesetzt.

\subsection{Algorithmen}
Im Laufe des Experiments werden drei verschiedene Klassifizierer eingesetzt, um das bestm\"ogliche Ergebnis in der Argumentklassifikation zu erreichen: eine Support Vector
Machine, ein NaiveBayes-Klassifkator und J48, eine OpenSource Java-Implementierung des C4.5-Decision-Tree-Algorithmus. Diese werden dann in der Evaluation gegeneinander 
verglichen um festzustellen, welcher Klassifizierer das beste Ergebnis erzielt. 

\subsection{Vergleichsgrundlagen}
Um einen Vergleich der Algorithmen ziehen zu k\"onnen, werden verschiedene Vergleichsgrundlagen verwendet. Die Daten aus dem PropBank-Korpus werden zun\"achst in Trainings-, Test- und 
Entwicklungsset aufgeteilt. Da diese bereits annotiert wurden, dienen sie somit als Goldstandard, der zur Berechnung von Precision, Recall und F1-Measure verwendet wird unter Verwendung
der verschiedenen Klassifizierer. Zus\"atzlich k\"onnen die Ergebnisse aus dem Paper \textit{Support Vector Learning for Semantic Argument Classiﬁcation} von Pradhan et al. mit denen des SVM-Klassifkators verglichen werden.


\section{Hauptteil}
\subsection{Zielsetzung}

Unser Ziel ist es Verbargumenten ihre semantischen Rollen zuzuweisen. Wir orientieren uns am PropBank Annotationsschema und
wollen die Instanzen in die sechs Klassen ARG[0-4] bzw. ARGM einteilen. Unsere Instanzen sind dabei die Gold Argumente des PropBank Penn Treebank Korpus.\\
\\
\begin{table}
\centering
\begin{tabular}{|c|l|}
\hline 
ARG0 & agent \\ 
\hline 
ARG1 & patient \\ 
\hline 
ARG2 & instrument, benefactive, attribute \\ 
\hline 
ARG3 & starting point, benefactive, attribute \\ 
\hline 
ARG4 & ending point \\ 
\hline 
ARGM & modifier \\ 
\hline 
\end{tabular}
\end{table}
 
\subsection{Umsetzung}

Wir extrahieren die Features und die Klasse aus unseren Instanzen des ProbBank Korpus wie folgt:
\begin{lstlisting}[frame=lines]
featureList = [...] # zu extrahierende Features
extArgList = []
for pbInstance in pbInstances :
   for pbArg in pbInstance.arguments :
      features = []
      for feature in featureList :
         extArgList.append(extFeature(feature, pbArg, pbInstance))
# write ARGInstances to file in ARFF
\end{lstlisting}

\subsubsection{Features}
\begin{itemize}
\item predicate : nominal \{ eat, pray, love, ... \}
\item path : nominal \{ NP$\uparrow$S$\downarrow$VP$\downarrow$VP, MD$\uparrow $VP$\uparrow$S$\downarrow$VP$\downarrow$VP, ... \}
\item phrase type : nominal \{ NP, MD, PP, ... \}
\item position : boolean \{ before, after \}
\item voice : boolean \{ active, passive \}
\item headword : nominal \{ food, god, Rose, ... \}
\item subcategorization : nominal \{ VP, NP, ... \}
\end{itemize}
siehe \cite{svm}

\subsection{Evaluation}
Um die Effizienz und Pr\"azision der angewandten Methoden zu ermitteln, erfolgt im Anschluss zu Feature-Extraktion und Experimenten eine Evaluation in zwei Stufen.\\
Zun\"achst werden SVM, NaiveBayes und J48 gegeneinander in Hinsicht auf Precision, Recall und F1-Measure verglichen. Da PropBank bereits golden annotiert ist, muss keine weitere manuelle Annotation als Vergleichsgrundlage erstellt werden. Weiterhin kann die Feature-Auswahl f\"ur jeden Algorithmus hinsichtlich ihrer Effizienz optimiert werden.\\
Optional k\"onnen insbesondere die Ergebnisse des SVM-Verfahrens gegen die des Papers \textit{Support Vector Learning for Semantic Argument Classiﬁcation}\cite{svm} verglichen werden.

\section{Ausblick}
Im n\"achsten geplanten Schritt wird die Feature-Extraktion durchgef\"uhrt. Die Funktionsvielfalt des NLTK wird hierbei \"au\ss{}erst hilfreich sein. Die anschlie\ss{}enden Experimente in Weka werden auf Grund der verschiedenen Algorithmen und ihrer jeweiligen Feature-Optimierung zeitaufw\"andig, dank Wekas \"ubersichtlichen Workflow jedoch gut umzusetzen sein.\\
Wir sind auf die Ergebnisse gespannt und hoffen, dass sie die Auswahl der effizientesten Methode zur semantischen Argumentklassifizierung in Zukunft erleichtern werden.

\bibliography{lit}{}
\bibliographystyle{plain}

\end{document}
